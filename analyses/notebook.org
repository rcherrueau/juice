# -*- org-confirm-babel-evaluate: nil; -*-
#+TITLE: Juice Notebook
#+AUTHOR: Marie Delavergne, Ronan-Alexandre Cherrueau
#+EMAIL: {firstname.lastname}@inria.fr
#+DATE: <2018>

#+LANGUAGE: en
#+OPTIONS: email:t
#+OPTIONS: ^:{}
#+OPTIONS: broken-links:mark

#+PROPERTY: header-args:python  :session default
#+PROPERTY: header-args:python+ :cache no
#+PROPERTY: header-args:python+ :var SNS_CONTEXT="notebook"
# #+PROPERTY: header-args:python+ :exports both  # export contains code + result see [[info:org#Exporting%20code%20blocks][info:org#Exporting code blocks]]
# #+PROPERTY: header-args:python+ :results output

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="timeline.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.16/css/jquery.dataTables.css">
#+HTML_HEAD: <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha256-3edrmyuQ0w65f8gfBsqowzjJe2iM6n0nKciPUp8y+7E=" crossorigin="anonymous"></script>
#+HTML_HEAD: <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.10.16/js/jquery.dataTables.js"></script>

#+BEGIN_abstract
OpenStack is the /de facto/ open source solution for the management of
cloud infrastructures and emerging solution for edge infrastructures
(/i.e./, hundreds of geo-distributed micro DCs composed of dozens of
servers). Unfortunately, some modules used by OpenStack services do
not fulfill edge infrastructure needs. For instance, core services use
the MariaDB Relational Database Management System (RDBMS) to store
their state. And a single MariaDB may not cope with thousands of
connections and intermittent network access. Therefore, is OpenStack
technically ready for the management of an edge infrastructure? To
find this out, we built /Juice/. Juice tests and evaluates the
performance of Relational Database Management Systems (RDBMS) in the
context of a distributed OpenStack with high latency network. The
following notebook presents performance tests results together with an
analyze of these results for three RDBMS: MariaDB, the OpenStack
default RDBMS; Galera, the multi-master clustering solution for
MariaDB; And CockroachDB, a NewSQL system that leverage distributed
consensus algorithms to scale.
#+END_abstract

* Experiments Params                                               :noexport:
List of all parameters considered in this notebook.

#+BEGIN_SRC python :results silent :exports none
RDBMSS = [ 'mariadb', 'galera', 'cockroachdb' ]
CSIZES = [ 3, 9, 45 ]
DELAYS = [ 0, 50, 150 ]
SCNS   = [
      "KeystoneBasic.authenticate_user_and_validate_token"
    , "KeystoneBasic.create_add_and_list_user_roles"
    , "KeystoneBasic.create_and_list_tenants"
    , "KeystoneBasic.get_entities"
    , "KeystoneBasic.create_user_update_password"
    , "KeystoneBasic.create_user_set_enabled_and_delete"
    , "KeystoneBasic.create_and_list_users"
]
OPS   = {
  "KeystoneBasic.authenticate_user_and_validate_token":
  [
    "keystone_v3.fetch_token",
    "keystone_v3.validate_token",
  ]
  , "KeystoneBasic.create_add_and_list_user_roles":
  [
    "keystone_v3.create_role",
    "keystone_v3.add_role",
    "keystone_v3.list_roles",
  ]
  , "KeystoneBasic.create_and_list_tenants":
  [
    "keystone_v3.create_project",
    "keystone_v3.list_projects",
  ]
  , "KeystoneBasic.get_entities":
  [
    "keystone_v3.create_project",
    "keystone_v3.create_user",
    "keystone_v3.create_role",
    "keystone_v3.get_project",
    "keystone_v3.get_user",
    "keystone_v3.get_role",
    "keystone_v3.list_services",
    "keystone_v3.get_services",
  ]
  , "KeystoneBasic.create_user_update_password":
  [
    "keystone_v3.create_user",
    "keystone_v3.update_user",
  ]
  , "KeystoneBasic.create_user_set_enabled_and_delete":
  [
    "keystone_v3.create_user",
    "keystone_v3.update_user",
    "keystone_v3.delete_user",
  ]
  , "KeystoneBasic.create_and_list_users":
  [
    "keystone_v3.create_user",
    "keystone_v3.list_users",
  ]
}
#+END_SRC

* Prelude                                                          :noexport:
#+BEGIN_SRC python :results silent
# From standard lib
from typing import (Dict, Union, Iterator,
                    Callable, List, Tuple,
                    TypeVar) # Type annoation

T = TypeVar('T')
U = TypeVar('U')

import glob                  # Unix style pathname
import itertools as itt
from operator import *
from functools import reduce
import re
import json
import textwrap

# Other libs
from dataclasses import dataclass   # Dataclass à la python 3.7
import objectpath                   # XPath for json
import pandas as pd                 # Data series analyses
import numpy as np
import matplotlib                   # Ploting
import matplotlib.pyplot as plt     # ^
import seaborn as sns               # ^
import functional                   # For my sanity
from functional import seq          # ^
from functional.util import compose # ^

# -- Utils
def normalize_series(scn: str, s: pd.Series) -> pd.Series:
    "Ensures that all operations of a scenario are present in `s`"
    operations = OPS.get(scn)
    news = pd.Series()
    for op in operations:
        if op in s.index:
            news = news.append(s.loc[[op]])
        else:
            news = news.append(pd.Series({op: np.nan}))
    return news

def make_series(scn: 'xp.scenario') -> pd.Series:
    "Builds a pd.Series with operations of `scn` in index"
    return pd.Series(np.nan, index=OPS.get(scn))

def make_cumulative_frequency(s: pd.Series) -> pd.Series:
    "Performed a Cumulative Frequency Analysis"
    cum_dist = np.linspace(0.,1.,len(s))
    return pd.Series(cum_dist, index=s.sort_values())

def success_rate(rally_values) -> float:
    "Returns success rate of a Rally scenario"
    JPATH_SUCCESS = '$.tasks[0].subtasks[0].workloads[0].statistics.durations.total.data.success'
    success = 0
    success_str = rally_values.execute(JPATH_SUCCESS)
    #
    if success_str.endswith('%'): # Success could be a percentage or
        # 'n/a'
        success = round(float(success_str[:-1]) / 100., 2)
        #
    return success

def df2orgtable(df: pd.DataFrame, index_name="") -> List[List[str]]:
    """
    Formats a 2d pandas DataFrame into in a org table.

    The optional `index_name` let you label indices.
    """
    columns = df.axes[1].values.tolist() # columns names
    indices = df.axes[0].values.tolist() # row labels
    rows    = df.values.tolist()         # rows
    # Put indeces in front of each row
    for index, r in enumerate(rows):
        r = list(map(lambda v: f'{v:.3f}', r))
        r.insert(0, indices[index])
        rows[index] = r
        #
    columns.insert(0, index_name)  # Id name in front of col names
    rows.insert(0, None)         # put a hline
    rows.insert(0, columns)      # put rows
    return rows

def df2orgtablestr(obj: Tuple['scenario', 'df_mean', 'df_std']) -> str:
    "Same as `df2orgtable` but produces a string"
    scn, df_mean, df_std = obj
    scn_short = textwrap.shorten((scn.replace('KeystoneBasic.', '')
                                  .replace('_', ' ')
                                  .title()),
                                 width=20,
                                 placeholder='...')
    df = df_mean.assign(std=df_std)
    res  = f'#+CAPTION: {scn}\n'
    res += f'#+NAME: tbl:{scn}\n'
    #
    for r in df2orgtable(df, scn_short):
        if r is None:
            res += "|--\n"
        else:
            res += "|" + reduce(add, intersperse_("|", map(str, r))) + "|\n"
            #
    return res

def xp2orgtable(xps: List['XP']) -> List[List[str]]:
    def xp2orgtablerow(xp) -> List[str]:
        "Format an `XP` into a org table row."
        latency = "LAN" if xp.latency == 0 else xp.latency * 2
        scn = xp.scenario.replace('KeystoneBasic.', '')
        fp = f'[[file:{xp.filepath}][...{xp.filepath[-11:]}]]'
        return [xp.cluster_size, latency, scn, xp.burst, xp.success, fp]
    # Make org table
    table = [ xp2orgtablerow(xp) for xp in xps ] # Body
    table.insert(0, None)                        # Hline
    table.insert(0, ["#Cluster", "RTT (ms)",     # Header
                     "Keystone Scenario",
                     "Burst", "Success", "Filepath"])
    return table

def _and(filters: List[Callable[[T], bool]]) -> Callable[[T], bool]:
    "Test a list of filter with AND"
    def __and(value: T) -> bool:
        for f in filters:
            if not f(value): return False
            #
        return True
    # Curry
    return __and

def df_add_const_column(df: pd.DataFrame, cvalue: T, cname: str) -> pd.DataFrame:
    "Adds column `cname` with value `cvalue` to `df`."
    nb_dfrows = df.index.size
    new_column = {cname: [cvalue for i in range(nb_dfrows)]}
    return df.assign(**new_column)

# -- Monkey patch PyFunctional with new combinator
def truth_map_t(f: Callable[[T], Union[None, U]]):
    """Standart `map` that fileters non `operator.truth` values.

    Equivalent to `seq(x).map(f).filter(operator.truth)`

    >>> seq([1, 2, 3, -1, 0, 4]).truth_map(lambda x: str(x) if x > 0 else None)
    ['1', '2', '3', '4']
    """
    fname = functional.transformations.name(f)
    return functional.transformations.Transformation(
        f'truth_map({fname})',
        lambda sequence: seq(sequence).map(f).filter(truth),
        None)

def on_value_t(f: Callable[[T], U]):
    """Applies f on the second element of a (k, v).

    >>> seq([("k1", 1), ("k2", 2)]).on_value(str)
    [("k1", "1"), ("k2", "2")]
    """
    fname = functional.transformations.name(f)
    return functional.transformations.Transformation(
        f'on_key({fname})',
        # lambda sequence: map(lambda kv: (kv[0], f(kv[1])), sequence),
        lambda sequence: seq(sequence).map(lambda kv: (kv[0], f(kv[1]))),
        None)

def map_on_value_t(f: Callable[[List[T]], List[U]]):
    """Maps f on the second element of a list of (k, [v]).

    >>> seq([("k1", [1, 1, 1]), ("k2", [2, 2, 2])]).map_on_value(str)
    [("k1", ["1", "1", "1"]), ("k2", ["2", "2", "2"])]
    """
    fname = functional.transformations.name(f)
    return functional.transformations.Transformation(
        f'map_on_value({fname})',
        # lambda sequence: map(lambda kv: (kv[0], seq(kv[1]).map(f)), sequence),
        lambda sequence: seq(sequence).map(lambda kv: (kv[0], seq(kv[1]).map(f))),
        None)

def push_t(e: T):
    """Add the element `e` in the sequence.

    >>> seq([1, 2]).push(0)
    [0, 1, 2]
    """
    def push(i: Iterator[any], e: any) -> Iterator[any]:
        l = list(i)
        l.insert(0, e)
        return l
    #
    ename = functional.transformations.name(e)
    return functional.transformations.Transformation(
        f'push({ename})',
        lambda sequence: push(sequence, e),
        None)

def intersperse_(delim: T, seq: Iterator[T]) -> Iterator[T]:
    it = iter(seq)
    yield next(it)
    for x in it:
        yield delim
        yield x

def intersperse_t(delim: T):
    ename = functional.transformations.name(delim)
    return functional.transformations.Transformation(
        f'intersperse({ename})',
        lambda sequence: intersperse(delim, sequence),
        None)

functional.pipeline.Sequence.truth_map = lambda self, f: self._transform(truth_map_t(f))
functional.pipeline.Sequence.on_value = lambda self, f: self._transform(on_value_t(f))
functional.pipeline.Sequence.map_on_value = lambda self, f: self._transform(map_on_value_t(f))
functional.pipeline.Sequence.push = lambda self, e: self._transform(push_t(e))
functional.pipeline.Sequence.intersperse = lambda self, e: self._transform(intersperse_t(e))
functional.pipeline.Sequence.__len__ = lambda self: self.len()
functional.pipeline.Sequence.head = lambda self: self.take(1).to_list().pop()

# plot config
sns.set()
sns.set_context(SNS_CONTEXT)
sns.set_palette("muted")
#+END_SRC

* Introduction
Definition of Discovery. What we try to do.

Take back HotEdge'18 paper and explain the bottom/up approach
(benefits: getting an edge IaaS manager for free by making OpenStack
natively distributed).

Tack back HotEdge'18 Central vs MultiRegion deployments

- Expected size of the cluseter (10000)
- WAN links
- Split brain
- Expected behaviors (read and write everywhere while maintaining ACID
  prop).

* Considered Databases
Description of databases, how they works, how they implements expected
behaviors of the previous section

** MariaDB
** Galera in multi-master replication mode.
** CockroachDB (/abbr./ CRDB).
* Considered OpenStack Services
Keystone

* Considered Rally Scenarios
[[https://rally.readthedocs.io/en/latest/][Rally]] is a testing benchmarking tool for OpenStack. Juice uses Rally
to evaluate how OpenStack control plane behaves at scale. This section
describes Rally scenarios that are considered in this experiment. The
description includes the ratio of reads and writes performed on the
database. For a transactional (OLTP) database, depending of the
reads/writes ratio, it could be better to choose one replication
strategy to another (i.e., replicate records on all of your nodes or
not).

** keystone/authenticate-user-and-validate-token
Description: authenticate and validate a keystone token.

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/authenticate-user-and-validate-token.yaml][samples/tasks/scenarios/keystone/authenticate-user-and-validate-token]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L111-L120][rally_openstack.scenarios.keystone.basic.AuthenticateUserAndValidateToken]]

List of keystone functionalities:
1. keystone_v3.fetch_token
2. keystone_v3.validate_token

%Reads/%Writes: 96.46/3.54

Number of runs: 20

** keystone/create-add-and-list-user-roles
Description: create user role, add it and list user roles for given
user.

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/create-add-and-list-user-roles.yaml][samples/tasks/scenarios/keystone/create-add-and-list-user-roles]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L214-L228][rally_openstack.scenarios.keystone.basic.CreateAddAndListUserRoles]]

List of keystone functionalities:
1. keystone_v3.create_role
2. keystone_v3.add_role
3. keystone_v3.list_roles

%Reads/%Writes: 96.22/3.78

Number of runs: 100

** keystone/create-and-list-tenants
Description: create a keystone tenant with random name and list all
tenants.

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/create-and-list-tenants.yaml][samples/tasks/scenarios/keystone/create-and-list-tenants]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L166-L181][rally_openstack.scenarios.keystone.basic.CreateAndListTenants]]

List of keystone functionalities:
1. keystone_v3.create_project
2. keystone_v3.list_projects

%Reads/%Writes: 92.12/7.88

Number of runs: 10

** keystone/get-entities
Description: get instance of a tenant, user, role and service by id's.
An ephemeral tenant, user, and role are each created. By default,
fetches the 'keystone' service.

List of keystone functionalities:
1. keystone_v3.create_project
2. keystone_v3.create_user
3. keystone_v3.create_role
   1) keystone_v3.list_roles
   2) keystone_v3.add_role
4. keystone_v3.get_project
5. keystone_v3.get_user
6. keystone_v3.get_role
7. keystone_v3.list_services
8. keystone_v3.get_services

%Reads/%Writes: 91.9/8.1

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/get-entities.yaml][samples/tasks/scenarios/keystone/get-entities]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L231-L261][rally_openstack.scenarios.keystone.basic.GetEntities]]

Number of runs: 100

** keystone/create-and-list-users
Description: create a keystone user with random name and list all
users.

List of keystone functionalities:
1. keystone_v3.create_user
2. keystone_v3.list_users

%Reads/%Writes: 92.05/7.95

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/create-add-and-list-user-roles.yaml][samples/tasks/scenarios/keystone/create-and-list-users]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L145-L163][rally_openstack.scenarios.keystone.basic.CreateAndListUsers]].

Number of runs: 100

** keystone/create-user-set-enabled-and-delete
Description: create a keystone user, enable or disable it, and delete
it.

List of keystone functionalities:
1. keystone_v3.create_user
2. keystone_v3.update_user
3. keystone_v3.delete_user

%Reads/%Writes: 91.07/8.93

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/create-user-set-enabled-and-delete.yaml][samples/tasks/scenarios/keystone/create-user-set-enabled-and-delete]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L75-L91][rally_openstack.scenarios.keystone.basic.CreateUserSetEnabledAndDelete]]

Number of runs: 100

** keystone/create-user-update-password
Description: create user and update password for that user.

List of keystone functionalities:
1. keystone_v3.create_user
2. keystone_v3.update_user

%Reads/%Writes: 89.79/10.21

Definition Code:
[[https://github.com/openstack/rally-openstack/blob/6158c1139c0a4d88cab74481c5cbfc8be398f481/samples/tasks/scenarios/keystone/create-user-update-password.yaml][samples/tasks/scenarios/keystone/create-user-update-password]]

Source Code:
[[https://github.com/openstack/rally-openstack/blob/b1ae405b7fab355f3062cdb56a5b187fc6f2907f/rally_openstack/scenarios/keystone/basic.py#L306-L320][rally_openstack.scenarios.keystone.basic.CreateUserUpdatePassword]]

Number of runs: 100

** A note about gauging the %reads/%writes ratio
The %reads/%writes ratio is computed on Mariadb. The gauging code
reads values of status variables ~Com_xxx~ that provide statement
counts over all connections (with ~xxx~ stands for ~SELECT~, ~DELETE~,
~INSERT~, ~UPDATE~, ~REPLACE~ statements). The SQL query that does
this job is available in listing [[lst:gauging-ratio-sql]] and returns the
total number of reads and writes since the database started. That SQL
query is called before and after the execution of one Rally scenario.
After and before values are then subtracted to compute the number of
reads and writes performed during the scenario and finally, compared
to compute the ratio.

#+CAPTION: Total number of reads and writes performed on
#+CAPTION: MariaDB since the last reboot
#+NAME: lst:gauging-ratio-sql
#+BEGIN_SRC sql :eval no
SELECT
  SUM(IF(variable_name = 'Com_select', variable_value, 0))
     AS `Total reads`,
  SUM(IF(variable_name IN ('Com_delete',
                           'Com_insert',
                           'Com_update',
                           'Com_replace'), variable_value, 0))
     AS `Total writes`
FROM  information_schema.GLOBAL_STATUS;
#+END_SRC

Note that %reads/%writes may be a little bit more in favor of reads
than what it is presented here because the following also takes into
account the creation/deletion of rally context. A basic Rally context
for a Keystone scenario is ~{"admin_cleanup@openstack":
["keystone"]}~. Not sure what does this context do exactly though,
maybe it only creates an admin user... This context may be extended by
other inserts specified in the scenario definition (under the
~context~ key; see scenario definition for
[[*keystone/create-add-and-list-user-roles][keystone/create-add-and-list-user-roles]]).

The Juice implementation for this gauging is available on GitHub at
[[https://github.com/rcherrueau/juice/blob/02af922a7c3221462d7106dfb2751b3be709a4d5/experiments/read-write-ratio.py][experiments/read-write-ratio.py]].

* Extract, Reify, Query Experiments and their Rally Results
The execution of a Rally scenario (such as those seen in the previous
section -- see [[*Considered Rally Scenarios][Considered Rally Scenarios]]) produces a json file. The
json file contains a list of entries (path ~workloads.data~): one for
each run of the scenario. An entry then retains the time (in second)
it takes to complete all Keystone operations involved in the Rally
scenario.

This notebook evaluate different database backends in the context of
an OpenStack for the edge on the basis of Rally benchmarking tool.
This section provides python facilities to extract and query Rally
results for latter analyses.

#+BEGIN_COMMENT
: for i in $(ls); do pushd $i; ls backup/*/rally-*.tar.gz | xargs -I '{}' tar -xf '{}'; popd; done
#+END_COMMENT

An archive with results of all experiments of this notebook is
available at TODO:url. Let's assume the ~XPS_PATH~ variable references
the path where this archive is extracted. In this archive, there is
results for experimentation on two databases engines: CRDB and Galera.
Results are in several json files, so listing [[lst:xp-paths]] define
accessors for all of them thanks to the [[https://docs.python.org/3/library/glob.html][~glob~]] python module. The
~glob~ module finds all paths that match a specified UNIX patterns.

#+CAPTION: Paths to Rally Json Results File.
#+NAME: lst:xp-paths
#+BEGIN_SRC python :results silent
# XP_PATHS = './marie/'
XP_PATHS = './ecotype/'
# MARIADB_XP_PATHS = glob.glob(XP_PATHS + 'mariadb-*/rally_home/*.json')
MARIADB_XP_PATHS = glob.glob(XP_PATHS + 'mariadb-*/rally_home/*.json')
# GALERA_XP_PATHS = glob.glob(XP_PATHS + 'galera-*/rally_home/*.json')
GALERA_XP_PATHS = glob.glob(XP_PATHS + 'galera-*/rally_home/*.json')
# CRDB_XP_PATHS = glob.glob(XP_PATHS + 'cockroachdb-*/rally_home/*.json')
CRDB_XP_PATHS = glob.glob(XP_PATHS + 'cockroachdb-*/rally_home/*.json')
#+END_SRC

** From Json files to Python Objects
A data class ~XP~ retains data of one experiment (i.e., name of the
rally scenario, name of database technology, ... -- see l.
[[(xp-dataclass-start)]] to [[(xp-dataclass-end)]] of listing [[lst:xp-dataclass]]
for the complete list). Reifing experiment data in a Python object
will help for the latter analyses. Whit a Python object, it is easier
to filer, sort, map, ... experiments.

#+CAPTION: Experiment Data Class.
#+NAME: lst:xp-dataclass
#+BEGIN_SRC python -r :results silent
@dataclass(frozen=True)
class XP:
    scenario: str     # Rally scenario name (ref:xp-dataclass-start)
    rdbms: str        # Name of the RDBMS (e,g, cockcroachdb, galera)
    filepath: str     # Filepath of the json file
    cluster_size: int # Size of the cluster
    latency: int      # Latency between nodes
    success: str      # Success rate (e.g., "100%")
    burst: bool       # Experiment performed during a burst
    dataframe: pd.DataFrame  # Results in a pandas 2d DataFrame (ref:xp-dataclass-end)
#+END_SRC

The ~XP~ data class comes with the ~make_xp~ function (see, lst.
[[lst:make_xp]]). It produces an ~XP~ object from an experiment file path
(i.e., Rally json file). Especially, it uses the python [[http://objectpath.org/][~objectpath~]]
module that provides a DSL to query Json documents (à la XPath) and
extract only interested data.

#+CAPTION: Builds an ~XP~ object from a Rally Json Result File.
#+NAME: lst:make_xp
#+BEGIN_SRC python -r :results silent :noweb no-export
def make_xp(rally_path: str) -> XP:
    # Find XP name in the `rally_path`
    RE_XP = r'(?:mariadb|galera|cockroachdb)-[a-zA-Z0-9\-]+'
    # Find XP params in the `rally_path` (e.g., cluster size, latency, ...)
    RE_XP_PARAMS = r'(?P<db>[a-z]+)-(?P<cluster_size>[0-9]+)-(?P<latency>[0-9]+)-(?P<burst>[TF]).*'
    # Json path to the rally scenario's name
    JPATH_SCN = '$.tasks[0].subtasks[0].title'
    # Json path to the rally status (crashed result are excluded)
    JPATH_STATUS  = '$.tasks[0].status'
    #
    <<lst:dataframe_per_operations>> (ref:dataframe_per_operations)
    #
    with open(rally_path) as rally_json:
        rally_values = objectpath.Tree(json.load(rally_json))
        rally_status = rally_values.execute(JPATH_STATUS)
        if rally_status == 'finished': # Remove crashed rally
            xp_info = re.match(RE_XP_PARAMS, re.findall(RE_XP, rally_path)[0]).groupdict()
            success = success_rate(rally_values)
            return XP(
                scenario = rally_values.execute(JPATH_SCN),
                filepath = rally_path,
                rdbms = xp_info.get('db'),
                cluster_size = int(xp_info.get('cluster_size')),
                latency = int(xp_info.get('latency')),
                success = success,
                burst = True if xp_info.get('burst') is 'T' else False,
                dataframe = dataframe_per_operations(rally_values) if success else None)
#+END_SRC

The [[(dataframe_per_operations)][~<<lst:dataframe_per_operations>>~]] is a placeholder for the
function that transforms Rally Json results in a pandas [[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame][~DataFrame~]]
for result analyses. The next section will say more on this. Right
now, focus on ~make_xp~. With ~make_xp~, transforming all Rally Jsons
into ~XP~ objects is as simple as mapping over all experiment paths
(see lst. [[lst:xps]]).

#+CAPTION: From Json Files to Python Objects.
#+NAME: lst:xps
#+BEGIN_SRC python :results silent
XPS = seq(MARIADB_XP_PATHS + GALERA_XP_PATHS + CRDB_XP_PATHS).truth_map(make_xp)
#+END_SRC

This notebook also comes with a bunch of predicate in its toolbelt
that ease the filtering and sorting of experiments. For instance a
function src_python[:exports code :eval no]{def is_crdb(xp: XP) ->
bool} only keeps CRDB experiments. And src_python[:exports code :eval
no]{def xp_csize_rtt_b_scn_order(xp: XP) -> str} returns a comparable
value to sort experiments. The complete list is available in the
source of this notebook.

#+BEGIN_SRC python :results silent :noweb no-export :exports none
# Memoization
<<lst:predicate>>
<<lst:hlq>>
<<lst:hlp>>

XPS = XPS.filter(with_success_rate(.01)).cache()
#+END_SRC

*** MariaDB experiments
Listing [[lst:mariadb_xps]] shows how to compute the list of experiments for
CockroachDB (~filter(is_crdb)~), sorted by the size of the cluster and
the Round Trip Time between nodes
(~order_by(xp_csize_rtt_b_scn_order)~). Table [[tab:crdb_xps]] presents the
results.

#+CAPTION: Access to MariaDB Experiments.
#+NAME: lst:mariadb_xps
#+BEGIN_SRC python :results silent
MARIADB_XPS = XPS.filter(is_mariadb).order_by(xp_csize_rtt_b_scn_order)
#+END_SRC

#+BEGIN_COMMENT
The ~xp2orgtable~ is a [[*Prelude][Prelude]] function that takes a list of ~XP~ and
formats them into an Org table as table [[tab:crdb_xps]].
#+END_COMMENT

#+HEADER: :colnames yes :hlines yes
#+NAME: lst:mariadb_xps_org
#+BEGIN_SRC python :results table :exports results :eval no
xp2orgtable(MARIADB_XPS)
#+END_SRC

*** CockroachDB experiments
Listing [[lst:crdb_xps]] shows how to compute the list of experiments for
CockroachDB (~filter(is_crdb)~), sorted by the size of the cluster and
the Round Trip Time between nodes
(~order_by(xp_csize_rtt_b_scn_order)~). Table [[tab:crdb_xps]] presents the
results.

#+CAPTION: Access to CockroachDB Experiments.
#+NAME: lst:crdb_xps
#+BEGIN_SRC python :results silent
CRDB_XPS = XPS.filter(is_crdb).order_by(xp_csize_rtt_b_scn_order)
#+END_SRC

#+BEGIN_COMMENT
The ~xp2orgtable~ is a [[*Prelude][Prelude]] function that takes a list of ~XP~ and
formats them into an Org table as table [[tab:crdb_xps]].
#+END_COMMENT

#+HEADER: :colnames yes :hlines yes
#+NAME: lst:crdb_xps_org
#+BEGIN_SRC python :results table :exports results :eval no
xp2orgtable(CRDB_XPS)
#+END_SRC

*** Galera experiments
Listing [[lst:galera_xps]] shows how to compute the list of experiments
for Galera (~filter(is_galera)~), sorted by the size of the cluster
and the Round Trip Time between nodes
(~order_by(xp_csize_rtt_b_scn_order)~). Table [[tab:galera_xps]] presents
the list of experiments.

#+CAPTION: Access to Galera Experiments.
#+NAME: lst:galera_xps
#+BEGIN_SRC python :results silent
GALERA_XPS = XPS.filter(is_galera).order_by(xp_csize_rtt_b_scn_order)
#+END_SRC

#+HEADER: :colnames yes :hlines yes
#+NAME: lst:galera_xps_org
#+BEGIN_SRC python :results table :exports results :eval no
xp2orgtable(GALERA_XPS)
#+END_SRC

** Query Rally Results
The Rally Json file contains values that give the scenario completion
time per keystone operations at a certain Rally run. These values must
be analyzed to evaluate which backend best suits for an OpenStack for
the edge. And a good python module to data analysis is [[https://pandas.pydata.org/][Pandas]]. Thus,
the function ~dataframe_per_operations~ (see
lst.[[lst:dataframe_per_operations]] -- part of [[lst:make_xp][~make_xp~]]) takes the Rally
json and returns a Pandas [[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame][~DataFrame~]].

#+CAPTION: Transform Rally Results into Pandas DataFrame.
#+NAME: lst:dataframe_per_operations
#+BEGIN_SRC python :results silent
# Json path to the completion time series
JPATH_SERIES = '$.tasks[0].subtasks[0].workloads[0].data[len(@.error) is 0].atomic_actions'
def dataframe_per_operations(rally_values: objectpath.Tree) -> pd.DataFrame:
    "Makes a 2d pd.DataFrame of completion time per keystone operations."
    return pd.DataFrame.from_items(
        items=(seq(rally_values.execute(JPATH_SERIES))
                 .flatten()
                 .group_by(itemgetter('name'))
                 .map_on_value(lambda it: it['finished_at'] - it['started_at'])))
#+END_SRC

The DataFrame is a table that lists all the completion times in second
for a certain Rally scenario. A column references a Keystone
operations and row labels (index) references the Rally run. Next
snippet (see, lst.[[lst:crdb_cltenants]]) is an example of the DataFrame
for the [[*keystone/create-and-list-tenants]["Creat and List Tenants"]] Rally scenario with ~9~ nodes in the
CRDB cluster and a ~LAN~ latency between each node. The ~lambda~ takes
the DataFrame and transforms it to add a "Total" column. Table
[[tab:crdb_cltenants]] presents the output of this DataFrame.


#+CAPTION: Access to the DataFrame of Rally ~create_and_list_tenants~.
#+NAME: lst:crdb_cltenants
#+BEGIN_SRC python :results silent
CRDB_CLTENANTS = (XPS
    .filter(is_keystone_scn('create_and_list_tenants'))
    .filter(when_cluster_size(9))
    .filter(is_crdb)
    .filter(compose(not_, is_burst))
    .filter(when_latency(0))
    .map(attrgetter('dataframe'))                    # Get DataFrame
    .map(lambda df: df.assign(Total=df.sum(axis=1))) # Add a Total Column
    .head())
#+END_SRC

#+HEADER: :rownames yes :colnames yes :hlines yes
#+NAME: lst:crdb_cltenants_org
#+BEGIN_SRC python :results table :exports results
df2orgtable(CRDB_CLTENANTS)
#+END_SRC

#+CAPTION: Entries for Rally ~create_and_list_tenants~,
#+CAPTION: 25 CRDB nodes, LAN latency.
#+NAME: tab:crdb_cltenants
#+RESULTS: lst:crdb_cltenants_org
|   | keystone_v3.create_project | keystone_v3.list_projects | Total |
|---+----------------------------+---------------------------+-------|
| 0 |                      0.140 |                     0.025 | 0.165 |
| 1 |                      0.134 |                     0.026 | 0.160 |
| 2 |                      0.132 |                     0.023 | 0.155 |
| 3 |                      0.133 |                     0.023 | 0.156 |
| 4 |                      0.130 |                     0.024 | 0.154 |
| 5 |                      0.129 |                     0.027 | 0.156 |
| 6 |                      0.143 |                     0.027 | 0.170 |
| 7 |                      0.133 |                     0.025 | 0.158 |
| 8 |                      0.135 |                     0.027 | 0.162 |
| 9 |                      0.141 |                     0.026 | 0.167 |

A pandas DataFrame presents the benefits of easily applying a wide
range of analyses. As an example, the following snippet (see,
lst.[[lst:crdb_cltenants_describe]]) computes the number of Rally runs
(i.e., ~count~), mean and standard deviation (i.e., ~mean~, ~std~),
the fastest and longest completion time (i.e., ~min~, ~max~), and the
25th, 50th and 75th percentiles (i.e., ~25%~, ~50%~, ~75%~). The
~transpose~ method transpose row labels (index) and columns. Table
[[tab:crdb_cltenants_describe]] presents the output of the analysis.

#+CAPTION: Analyse the DataFrame of Rally ~create_and_list_tenants~.
#+NAME:lst:crdb_cltenants_describe
#+BEGIN_SRC python :results silent
CRDB_CLTENANTS_ANALYSIS = CRDB_CLTENANTS.describe().transpose()
#+END_SRC

#+HEADER: :rownames yes :colnames yes :hlines yes
#+NAME:lst:crdb_cltenants_describe_org
#+BEGIN_SRC python :results table :exports results
df2orgtable(CRDB_CLTENANTS_ANALYSIS)
#+END_SRC

#+CAPTION: Analyses of Rally ~create_and_list_tenants~,
#+CAPTION: 25 CRDB nodes, LAN latency.
#+NAME:tab:crdb_cltenants_describe
#+RESULTS: lst:crdb_cltenants_describe_org
|                            |  count |  mean |   std |   min |   25% |   50% |   75% |   max |
|----------------------------+--------+-------+-------+-------+-------+-------+-------+-------|
| keystone_v3.create_project | 10.000 | 0.135 | 0.005 | 0.129 | 0.132 | 0.133 | 0.138 | 0.143 |
| keystone_v3.list_projects  | 10.000 | 0.025 | 0.002 | 0.023 | 0.025 | 0.026 | 0.027 | 0.027 |
| Total                      | 10.000 | 0.160 | 0.005 | 0.154 | 0.156 | 0.159 | 0.164 | 0.170 |

* Heavy Lifting                                                    :noexport:
Functions that do the heavy lifting for the rest of this notebook.

** Predicates
#+NAME: lst:predicate
#+BEGIN_SRC python :results silent
def is_crdb(xp: XP) -> bool:
    "Filter for CRDB experiment."
    return xp.rdbms == 'cockroachdb'

def is_galera(xp: XP) -> bool:
    "Filter for Galera experiment."
    return xp.rdbms == 'galera'

def is_mariadb(xp: XP) -> bool:
    "Filter for MariaDB experiment."
    return xp.rdbms == 'mariadb'

def is_burst(xp: XP) -> bool:
    "Filter for bursted experiment."
    return xp.burst

def is_keystone_scn(scn: str) -> bool:
    "Filter for keystone scenario `scn`."
    return lambda xp: xp.scenario == 'KeystoneBasic.' + scn

def when_latency(lat: int) -> Callable[[XP], bool]:
    "Filter for latence `lat`."
    return lambda xp: xp.latency == lat

def when_cluster_size(csize: int) -> Callable[[XP], bool]:
    "Filter for cluster size `csize`."
    return lambda xp: xp.cluster_size == csize

def with_success_rate(rate: float) -> Callable[[XP], bool]:
    "Filter for cluster size `csize`."
    return lambda xp: xp.success >= rate

def xp_csize_rtt_b_scn_order(xp: XP) -> str:
    """
    Returns a comparable value to sort experiments.

    The sort is made on
    1. The database type (CRDB or Galera)
    2. Size of the cluster
    3. Latency
    4. No Burst, Burst
    5. Rally scenario's name
    """
    # Format String Syntax
    # https://docs.python.org/2/library/string.html#format-examples
    return f'{xp.rdbms}-{xp.cluster_size:0>3}-{xp.latency:0>3}-{xp.burst}-{xp.scenario}'

#+END_SRC

** High level Queries
#+NAME: lst:hlq
#+BEGIN_SRC python :results silent
def add_total_column(df: pd.DataFrame) -> pd.DataFrame:
    "Adds the Total column that sum values of all columns"
    return df.assign(Total=df.sum(axis='columns'))

def filter_percentile(q: float) -> Callable[[pd.DataFrame], pd.DataFrame]:
    "Removes values upper than percentile `q` of a Rally based DataFrame"
    #
    def find_column_with_biggest_impact(df: pd.DataFrame) -> str:
        "Returns the column's name with values that most impacts the plot crushing"
        return df.std().idxmax()
    # Curry
    def _filter(df: pd.DataFrame) -> pd.DataFrame:
        df_with_total = add_total_column(df)
        percentile = df_with_total.quantile(q)['Total']
        new_df = df_with_total[df_with_total['Total'] < percentile]
        return new_df.drop('Total', axis='columns')
    #
    return _filter

def set_xp_df(xp: XP, new_df: pd.DataFrame) -> XP:
    "Sets dataframe `new_df` of XP `xp`"
    return XP(scenario=xp.scenario,
              filepath=xp.filepath,
              rdbms=xp.rdbms,
              cluster_size=xp.cluster_size,
              latency=xp.latency,
              success=xp.success,
              burst=xp.burst,
              dataframe=new_df)

def reify_in_xpdf(attr: str) -> Callable[[XP], XP]:
    "Pushes `XP.attr` attribute value into `XP.dataframe` under `attr` column"
    # Curry
    def _push(xp: XP) -> XP:
        column_value = attrgetter(attr)(xp)
        column_name  = attr
        df_with_new_col = df_add_const_column(xp.dataframe, column_value, column_name)
        return set_xp_df(xp, df_with_new_col)
    #
    return _push

def results_per_scn_attr(attr: str, xps: List[XP]) -> List[
        Tuple[str, pd.DataFrame, pd.DataFrame]]:
    return (xps
            # Index XPs by scenario: [(scenario, [xps-csize{3/25/45}-lat0])]
            .group_by(attrgetter('scenario'))
            # Push values of `xp.attr` and `xp.rdbms` in the
            # dataframe. And only keep values under the 90th
            # percentile.
            .map_on_value(reify_in_xpdf(attr))
            .map_on_value(reify_in_xpdf('rdbms'))
            .map_on_value(attrgetter('dataframe'))
            .map_on_value(filter_percentile(.95))
            # Get one big DataFrame per scenario:
            # [(scenario, df{keystone.op1, keystone.op2, ..., cluster_size, rdbms})]
            .on_value(lambda dfs: pd.concat(dfs.to_list()))
            # Groupe by `xp.rdbms` and `xp.attr`, to compute the mean
            # and std of each group:
            .on_value(lambda df: df.groupby(['rdbms', attr]))
            # Returns this as a triplet: (scn, df_mean, df_std)
            .map(lambda scn_gdf: (
                scn_gdf[0],
                scn_gdf[1].aggregate('mean'),
                scn_gdf[1].apply(lambda df: df.sum(axis=1).std())))
          )

def scn_mean_std(obj: Tuple['scenario', pd.DataFrame]) -> Tuple[
        'scenario', pd.DataFrame, pd.DataFrame]:
    scn, gdf = obj
    return (scn, gdf.aggregate('mean'), gdf.apply(lambda df: df.sum(axis=1).std()))
#+END_SRC

** Ploting results
#+NAME: lst:ploting
#+BEGIN_SRC python :results silent
def series_stackedbar_plot(scn: 'xp.scenario',
                           ops_std: Dict['xp.attr', Union[Tuple['pd.Series_with_success', float], None]],
                           ax: matplotlib.axes.Axes):
    """Vertical bar plot of a dict of pd.Series.

    Vertiacal bar plot pushses all series of one dict key in one bar
    (e.g., one bar for a cluster size of 3, one bar for a cluster size
    of 9, and one bar for a cluster size of 45) . The bar is divided
    in mutiple parts that depict the value of each operation (e.g.,
    keystone.create_user and keystone.update_user).
    """
    # Bars in the plot are keys in the Dict (eg, 3, 25, 45 or 0, 50,
    # 150).
    bars = list(ops_std.keys())
    nb_bar = len(bars)
    # Size of a bar is 100% of the x view divided by the number of bar.
    bar_width = 1.0/nb_bar
    bar_index = [ i * bar_width for i in range(nb_bar) ]
    # Put on tick per bar on x axis
    ax.set_xticks(bar_index)
    # Operations (index) in the Series, e.g.,
    # keystone_v3.create_project, keystone_v3.create_user, ...
    operations = OPS[scn]
    #
    normalized_ops_std = {}
    for attr, v in ops_std.items():
        if v:
            operation_series = normalize_series(scn, v[0])
            success = v[0].loc["success"]
            std = v[1]
        else:
            operation_series = make_series(scn)
            success = 0
            std = 0
        #
        normalized_ops_std.setdefault(attr, (operation_series, success, std))
    #
    # Make a datafram with results, e.g.,
    #                                   3         9         45
    # keystone_v3.create_project  0.137284  0.145858  0.154108
    # keystone_v3.create_user     0.176240  0.183208  0.196593
    # keystone_v3.create_role     0.031082  0.031126  0.034259
    # keystone_v3.get_project     0.020774  0.020956  0.022913
    # keystone_v3.get_user        0.020317  0.020496  0.022833
    # keystone_v3.get_role        0.020130  0.020629  0.022903
    # keystone_v3.list_services   0.023072  0.023743  0.026078
    # keystone_v3.get_services    0.020144  0.020214  0.022274
    df  = pd.DataFrame.from_dict({ k: s for k, (s, succ, std) in normalized_ops_std.items() })
    success = [ succ for k, (s, succ, std) in normalized_ops_std.items() ]
    std = [ std for k, (s, succ, std) in normalized_ops_std.items() ]
    # Plots rows one after the other (stacked). The plot is
    # made by calling `ax.bar` with all values of the first row,
    # then, all values of the second row, and so on, until the last
    # row.
    for irow, row in enumerate(operations):
        # Stack values on top of the previous row
        previous_row = None if irow == 0 else df.loc[:df.index[irow - 1]].sum(axis='index')
        # Print total standard deviation on the last element of the stack
        # yerr = None if row != operations[-1] else std
        yerr = None
        # Plot
        rects = ax.bar(bar_index, df.loc[row].values, bar_width,
                       bottom=previous_row, yerr=yerr, label=row)
    #
    # Add success rate on top of the last row
    for irect, rect in enumerate(rects):
        x = rect.get_x() + rect.get_width()*0.5
        y = rect.get_y() + rect.get_height()*1.01
        succ = int(round(success[irect] * 100))
        ax.text(x, y, f'{succ}%', ha='center', va='bottom')
    #
    ax.set_xticklabels(bars)

csize_plot("%s Completion Time (s)",
           series_stackedbar_plot,
           'imgs/cluster-size-impact-nburst.svg',
           # Compute the mean and the std of the results
           (XPS_CSIZE_NBURST
            .on_value(lambda df: (df.median(), df.sum(axis=1).std()))
            .to_dict()))

def series_linear_plot(scn: 'xp.scenario',
                       cfs: Dict['xp.attr', Union[pd.Series, None]],
                       ax: matplotlib.axes.Axes):
    # Lines in the plot are keys in the Dict (eg, 3, 25, 45 or 0, 50,
    # 150).
    lines = list(cfs.keys())
    # Plots lines one after the other. made by calling `ax.bar` with
    # all values of the experiment, then, all values of the second,
    # and so on, until the last row.
    for attr, cf in cfs.items():
        normalized_cf = cf if cf is not None else pd.Series(np.nan, index=range(10))
        ax.plot(normalized_cf, drawstyle='steps', label=attr)
#+END_SRC

* Cluster Size Impact
In this test, the size of the database cluster varies between 3, 9
and 45. The test evaluates how the completion time of Rally scenarios
varies, depending of the size of the cluster.

- TODO: describe the experimentation protocol
- TODO: Link the github juice code

** Plot                                                            :noexport:
#+BEGIN_SRC python :results silent
def csize_plot(ytitle: str,
               plot: Callable[['xp.scenario',
                               Dict['xp.cluster_size', T],
                               matplotlib.axes.Axes], None],
               filepath: str,
               xps: Dict[Tuple['xp.scenario', 'xp.rdbms', 'xp.cluster_size'], T],
               legend: Union['bottom-out', 'all'] = 'bottom-out'):
    subfig_width  = 4    # inch
    subfig_height = 4    # inch
    nscns  = len(SCNS)   # Number of scenarios
    nrdbms = len(RDBMSS) # Number of rdbms
    fig, axs = plt.subplots(nrows=nrdbms,
                            ncols=nscns,
                            figsize=(subfig_width  * nscns,
                                     subfig_height * nrdbms),
                            tight_layout=True,
                            sharex='col',
                            sharey='col')
    # Subplots for sncs x rdmbss
    scns_rdbmss = [ (s, r) for s in enumerate(SCNS) for r in enumerate(RDBMSS) ]
    for (iscn, scn), (irdbms, rdbms) in scns_rdbmss:
        # Get subplot for `scn` and `rdbms`
        ax = axs[irdbms][iscn]
        # Get all experiments for `scn` and `rdbms`, indexed by the
        # cluster size
        csize_xps = { csize : xps.get((scn, rdbms, csize), None) for csize in CSIZES}
        # Plot
        plot(scn, csize_xps, ax)
        # Only print y label for the first column
        if iscn == 0:
            ax.set_ylabel(ytitle % rdbms.title())
        #
        # Only print scenario name for the first row
        if irdbms == 0:
            fig_title = textwrap.shorten((scn.replace('KeystoneBasic.', '')
                                             .replace('_', ' ')
                                             .title()),
                                         width=30,
                                         placeholder='...')
            ax.set_title(fig_title, loc='left')
        #
        # Remove x label except for the last row
        if irdbms != len(RDBMSS) - 1:
            plt.setp(ax.get_xticklabels(), visible=False)
        #
        # Legend at the bottom of the view on the last row
        if legend == 'bottom-out' and irdbms == len(RDBMSS) - 1:
            box = ax.get_position()
            ax.set_position([box.x0, box.y0 + box.height * 0.1,
                             box.width, box.height * 0.9])
            ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1))
        #
        # Legend on all plot
        if legend == 'all':
            ax.legend()
    #
    #
    fig.align_labels()
    fig.savefig(filepath)
    return filepath
#+END_SRC

** Non Burst
#+BEGIN_SRC python :results silent
XPS_CSIZE_NBURST = (XPS
                    .filter(when_latency(0))
                    .filter(compose(not_, is_burst))
                    # Index XPs by scenario: [((scenario, rdbms, csize), [xps-csize{3/9/45}-lat0])]
                    .group_by(lambda xp: (xp.scenario, xp.rdbms, xp.cluster_size))
                    # Only keep values under the 95th percentile.
                    .map_on_value(reify_in_xpdf('success'))
                    .map_on_value(attrgetter('dataframe'))
                    .map_on_value(filter_percentile(.9))
                    # Get one big DataFrame -- concat all burst
                    # results:
                    # [((scenario, rdbms, csize), df{keystone.op1, keystone.op2, ...})]
                    .on_value(lambda dfs: pd.concat(dfs.to_list())))
#+END_SRC

*** Mean of Keystone Operations
#+NAME: lst:xps_csize_nburst
#+BEGIN_SRC python :results file :exports results
def debug(df):
    print(df)
    return df

csize_plot("%s Completion Time (s)",
           series_stackedbar_plot,
           'imgs/cluster-size-impact-nburst.svg',
           # Compute the mean and the std of the results
           (XPS_CSIZE_NBURST
            .on_value(lambda df: (df.median(), df.sum(axis=1).std()))
            .to_dict()))
#+END_SRC

#+CAPTION: Impact of the Cluster Size on the Completion Time (one Rally).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_csize_nburst
[[file:imgs/cluster-size-impact-nburst.svg]]

*** Cumulative Frequency Distribution
#+NAME: lst:xps_csize_nburst_cdf
#+BEGIN_SRC python :results file :exports results
csize_plot("%s",
           series_linear_plot,
           'imgs/cluster-size-impact-nburst-cdf.svg',
           # Sum operations of each iteration, and then compute de
           # cumulative frequency
           (XPS_CSIZE_NBURST
            .on_value(lambda df: df.drop('success', axis='columns'))
            .on_value(lambda df: df.sum(axis='columns'))
            .on_value(make_cumulative_frequency)
            .to_dict()),
           legend='all')
#+END_SRC

#+CAPTION: Impact of the Cluster Size on the
#+CAPTION: Completion Time (Cumulative Frequency).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_csize_nburst_cdf
[[file:imgs/cluster-size-impact-nburst-cdf.svg]]

*** Raw Results                                                    :noexport:
#+BEGIN_SRC python :results raw :exports results
# XPS_CSIZE_NBURST.map(scn_mean_std).map(df2orgtablestr).make_string('\n')
#+END_SRC

#+RESULTS:
('KeystoneBasic.create_and_list_tenants', 'galera', 3)
('KeystoneBasic.create_and_list_tenants', 'galera', 3)
#+CAPTION: KeystoneBasic.authenticate_user_and_validate_token
#+NAME: tbl:KeystoneBasic.authenticate_user_and_validate_token
| Authenticate User... | keystone_v3.fetch_token | keystone_v3.validate_token |   std |
|----------------------+-------------------------+----------------------------+-------|
| ('cockroachdb', 3)   |                   0.054 |                      0.083 | 0.003 |
| ('cockroachdb', 9)   |                   0.058 |                      0.090 | 0.005 |
| ('cockroachdb', 45)  |                   0.059 |                      0.093 | 0.003 |
| ('galera', 3)        |                   0.046 |                      0.068 | 0.005 |
| ('galera', 9)        |                   0.045 |                      0.070 | 0.004 |
| ('galera', 45)       |                   0.049 |                      0.077 | 0.010 |
| ('mariadb', 3)       |                   0.042 |                      0.067 | 0.003 |
| ('mariadb', 9)       |                   0.045 |                      0.067 | 0.003 |
| ('mariadb', 45)      |                   0.048 |                      0.077 | 0.006 |

#+CAPTION: KeystoneBasic.create_add_and_list_user_roles
#+NAME: tbl:KeystoneBasic.create_add_and_list_user_roles
|Create Add And...|keystone_v3.create_role|keystone_v3.add_role|keystone_v3.list_roles|std|
|--
|('cockroachdb', 3)|0.104|0.032|0.032|0.024|
|('cockroachdb', 9)|0.106|0.032|0.032|0.023|
|('cockroachdb', 45)|0.121|0.035|0.035|0.028|
|('galera', 3)|0.071|0.026|0.026|0.005|
|('galera', 9)|0.074|0.028|0.027|0.004|
|('galera', 45)|0.133|0.076|0.048|0.090|
|('mariadb', 3)|0.077|0.025|0.026|0.015|
|('mariadb', 9)|0.080|0.025|0.026|0.017|
|('mariadb', 45)|0.077|0.027|0.028|0.011|

#+CAPTION: KeystoneBasic.create_and_list_tenants
#+NAME: tbl:KeystoneBasic.create_and_list_tenants
|Create And List...|keystone_v3.create_project|keystone_v3.list_projects|std|
|--
|('cockroachdb', 3)|0.122|0.023|0.002|
|('cockroachdb', 9)|0.134|0.025|0.005|
|('cockroachdb', 45)|0.141|0.026|0.004|
|('galera', 3)|0.104|0.020|0.004|
|('galera', 9)|0.105|0.020|0.003|
|('galera', 45)|0.129|0.024|0.014|
|('mariadb', 3)|0.100|0.020|0.004|
|('mariadb', 9)|0.103|0.020|0.003|
|('mariadb', 45)|0.110|0.022|0.004|

#+CAPTION: KeystoneBasic.create_and_list_users
#+NAME: tbl:KeystoneBasic.create_and_list_users
|Create And List...|keystone_v3.create_user|keystone_v3.list_users|std|
|--
|('cockroachdb', 3)|0.139|0.084|0.015|
|('cockroachdb', 9)|0.147|0.085|0.019|
|('cockroachdb', 45)|0.150|0.084|0.009|
|('galera', 3)|0.111|0.058|0.012|
|('galera', 9)|0.114|0.059|0.012|
|('mariadb', 3)|0.108|0.057|0.014|
|('mariadb', 9)|0.111|0.056|0.012|
|('mariadb', 45)|0.120|0.057|0.011|

#+CAPTION: KeystoneBasic.create_user_set_enabled_and_delete
#+NAME: tbl:KeystoneBasic.create_user_set_enabled_and_delete
|Create User Set...|keystone_v3.create_user|keystone_v3.update_user|keystone_v3.delete_user|std|
|--
|('cockroachdb', 3)|0.139|0.081|0.770|3.037|
|('cockroachdb', 9)|0.156|0.105|0.796|1.976|
|('cockroachdb', 45)|0.161|0.111|0.562|1.046|
|('galera', 3)|0.118|0.063|0.108|0.025|
|('galera', 9)|0.117|0.064|0.111|0.015|
|('mariadb', 3)|0.114|0.062|0.108|0.022|
|('mariadb', 9)|0.114|0.058|0.104|0.015|
|('mariadb', 45)|0.137|0.068|0.112|0.032|

#+CAPTION: KeystoneBasic.create_user_update_password
#+NAME: tbl:KeystoneBasic.create_user_update_password
|Create User...|keystone_v3.create_user|keystone_v3.update_user|std|
|--
|('cockroachdb', 3)|0.132|1.253|6.381|
|('cockroachdb', 9)|0.184|0.554|2.726|
|('cockroachdb', 45)|0.181|0.567|2.317|
|('galera', 3)|0.112|0.065|0.006|
|('galera', 9)|0.115|0.069|0.006|
|('mariadb', 3)|0.108|0.063|0.006|
|('mariadb', 9)|0.111|0.065|0.008|
|('mariadb', 45)|0.123|0.069|0.009|

#+CAPTION: KeystoneBasic.get_entities
#+NAME: tbl:KeystoneBasic.get_entities
|Get Entities|keystone_v3.create_project|keystone_v3.create_user|keystone_v3.create_role|keystone_v3.get_project|keystone_v3.get_user|keystone_v3.get_role|keystone_v3.list_services|keystone_v3.get_services|std|
|--
|('cockroachdb', 3)|0.137|0.176|0.031|0.021|0.020|0.020|0.023|0.020|0.011|
|('cockroachdb', 9)|0.146|0.183|0.031|0.021|0.020|0.021|0.024|0.020|0.009|
|('cockroachdb', 45)|0.154|0.197|0.034|0.023|0.023|0.023|0.026|0.022|0.011|
|('galera', 3)|0.114|0.142|0.025|0.018|0.018|0.017|0.019|0.017|0.009|
|('galera', 9)|0.117|0.145|0.025|0.018|0.018|0.018|0.020|0.018|0.009|
|('galera', 45)|0.277|0.357|0.363|0.037|0.030|0.028|0.027|0.020|1.236|
|('mariadb', 3)|0.110|0.137|0.023|0.018|0.018|0.018|0.019|0.017|0.011|
|('mariadb', 9)|0.112|0.141|0.023|0.018|0.018|0.018|0.019|0.017|0.010|
|('mariadb', 45)|0.121|0.150|0.026|0.020|0.020|0.020|0.021|0.020|0.011|

** Burst
#+NAME: lst:xps_csize_burst
#+BEGIN_SRC python :results silent
XPS_CSIZE_BURST = (XPS
                   .filter(when_latency(0))
                   .filter(is_burst)
                   .group_by(lambda xp: (xp.scenario, xp.rdbms, xp.cluster_size))
                   .map_on_value(reify_in_xpdf('success'))
                   .map_on_value(attrgetter('dataframe'))
                   .map_on_value(filter_percentile(.9))
                   .on_value(lambda dfs: pd.concat(dfs.to_list())))
#+END_SRC

*** Mean of Keystone Operations
#+NAME: lst:xps_csize_burst
#+BEGIN_SRC python :results file :exports results
csize_plot("%s Completion Time (s)",
           series_stackedbar_plot,
           'imgs/cluster-size-impact-burst.svg',
           # Compute the mean and the std of the results
           (XPS_CSIZE_BURST
            .on_value(lambda df: (df.mean(), df.sum(axis=1).std()))
            .to_dict()))
#+END_SRC

#+CAPTION: Impact of the Cluster Size on the Completion Time (burst).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_csize_burst
[[file:imgs/cluster-size-impact-burst.svg]]

*** Cumulative Frequency Distribution
#+NAME: lst:xps_csize_burst_cdf
#+BEGIN_SRC python :results file :exports results
csize_plot("%s  (s)",
           series_linear_plot,
           'imgs/cluster-size-impact-burst-cdf.svg',
           # Sum operations of each iteration, and then compute de
           # cumulative frequency
           (XPS_CSIZE_BURST
            .on_value(lambda df: df.drop('success', axis='columns'))
            .on_value(lambda df: df.sum(axis='columns'))
            .on_value(make_cumulative_frequency)
            .to_dict()),
           legend='all')
#+END_SRC

#+CAPTION: Impact of the Cluster Size on the
#+CAPTION: Completion Time (Cumulative Frequency -- Burst).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_csiz_burst_cdf
[[file:imgs/cluster-size-impact-burst-cdf.svg]]

* Delay Impact
In this test, the size of the database cluster is 9 and the latency
varies between LAN, 100 and 300 ms of RTT. The test evaluates how the
completion time of Rally scenarios varies, depending of RTT between
nodes of the swarm.

- TODO: describe the experimentation protocol
- TODO: Link the github juice code

** Plot                                                            :noexport:
#+BEGIN_SRC python :results silent
def delay_plot(ytitle: str,
               plot: Callable[['xp.scenario',
                               Dict['xp.delay', T],
                               matplotlib.axes.Axes], None],
               filepath: str,
               xps: Dict[Tuple['xp.scenario', 'xp.rdbms', 'xp.delay'], T],
               legend: Union['bottom-out', 'all'] = 'bottom-out'):
    subfig_width  = 4    # inch
    subfig_height = 4    # inch
    nscns  = len(SCNS)   # Number of scenarios
    nrdbms = len(RDBMSS) # Number of rdbms
    fig, axs = plt.subplots(nrows=nrdbms,
                            ncols=nscns,
                            figsize=(subfig_width  * nscns,
                                     subfig_height * nrdbms),
                            tight_layout=True,
                            sharex='col',
                            sharey='col')
    # Subplots for sncs x rdmbss
    scns_rdbmss = [ (s, r) for s in enumerate(SCNS) for r in enumerate(RDBMSS) ]
    for (iscn, scn), (irdbms, rdbms) in scns_rdbmss:
        # Get subplot for `scn` and `rdbms`
        ax = axs[irdbms][iscn]
        # Get all experiments for `scn` and `rdbms`, indexed by the
        # delay
        delay_xps = { delay : xps.get((scn, rdbms, delay), None) for delay in DELAYS}
        # Plot
        plot(scn, delay_xps, ax)
        # Only print y label for the first column
        if iscn == 0:
            ax.set_ylabel(ytitle % rdbms.title())
        #
        # Only print scenario name for the first row
        if irdbms == 0:
            fig_title = textwrap.shorten((scn.replace('KeystoneBasic.', '')
                                             .replace('_', ' ')
                                             .title()),
                                         width=30,
                                         placeholder='...')
            ax.set_title(fig_title, loc='left')
        #
        # Remove x label except for the last row
        if irdbms != len(RDBMSS) - 1:
            plt.setp(ax.get_xticklabels(), visible=False)
        #
        # Legend at the bottom of the view on the last row
        if legend == 'bottom-out' and irdbms == len(RDBMSS) - 1:
            box = ax.get_position()
            ax.set_position([box.x0, box.y0 + box.height * 0.1,
                             box.width, box.height * 0.9])
            ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1))
        #
        # Legend on all plot
        if legend == 'all':
            ax.legend()
    #
    fig.align_labels()
    fig.savefig(filepath)
    return filepath
#+END_SRC

** Throughput Expectations
See [[http://enos.irisa.fr/html/wan_g5k/cpt10/][cpt10-lat*-los0/*.stats]] for raw measures.

#+NAME: throughput-data
#+CAPTION: Throughput Expectations
| Latency (ms) | Throughput (Mbits/s) |
|--------------+----------------------|
|     0.150614 |          9410.991784 |
|    20.000000 |          1206.381685 |
|    50.000000 |           480.173601 |
|   100.000000 |           234.189943 |
|   200.000000 |           115.890071 |

** Non Burst
#+BEGIN_SRC python :results silent
XPS_DELAY_NBURST = (XPS
                    .filter(when_cluster_size(9))
                    .filter(compose(not_, is_burst))
                    .group_by(lambda xp: (xp.scenario, xp.rdbms, xp.latency))
                    .map_on_value(attrgetter('dataframe'))
                    .map_on_value(filter_percentile(.9))
                    .on_value(lambda dfs: pd.concat(dfs.to_list())))
#+END_SRC

*** Mean of Keystone Operations
#+NAME: lst:xps_delay_nburst
#+BEGIN_SRC python :results file :exports results
delay_plot("%s Completion Time (s)",
           series_stackedbar_plot,
           'imgs/delay-impact-nburst.svg',
           # Compute the mean and the std of the results
           XPS_DELAY_NBURST.on_value(lambda df: (df.median(), df.sum(axis=1).std())).to_dict())
#+END_SRC

#+CAPTION: Impact of the Delay on the Completion Time (one Rally).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_delay_nburst
[[file:imgs/delay-impact-nburst.svg]]

*** Cumulative Frequency Distribution
#+NAME: lst:xps_delay_nburst_cdf
#+BEGIN_SRC python :results file :exports results
delay_plot("%s",
           series_linear_plot,
           'imgs/delay-impact-nburst-cdf.svg',
           (XPS_DELAY_NBURST
            .on_value(lambda df: df.sum(axis='columns'))
            .on_value(make_cumulative_frequency)
            .to_dict()),
           legend='all')
#+END_SRC

#+CAPTION: Impact of the Delay on the
#+CAPTION: Completion Time (Cumulative Frequency).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_delay_nburst_cdf
[[file:imgs/delay-impact-nburst-cdf.svg]]

** Burst
#+BEGIN_SRC python :results silent
XPS_DELAY_BURST = (XPS
                   .filter(when_cluster_size(9))
                   .filter(is_burst)
                   .group_by(lambda xp: (xp.scenario, xp.rdbms, xp.latency))
                   .map_on_value(attrgetter('dataframe'))
                   .map_on_value(filter_percentile(.9))
                   .on_value(lambda dfs: pd.concat(dfs.to_list())))
#+END_SRC

*** Mean of Keystone Operations
#+NAME: lst:xps_delay_burst
#+BEGIN_SRC python :results file :exports results
delay_plot("%s Completion Time (s)",
           series_stackedbar_plot,
           'imgs/delay-impact-burst.svg',
           # Compute the mean and the std of the results
           XPS_DELAY_BURST.on_value(lambda df: (df.mean(), df.sum(axis=1).std())).to_dict())
#+END_SRC

#+CAPTION: Impact of the Delay on the Completion Time (Burst).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_delay_burst
[[file:imgs/delay-impact-burst.svg]]

*** Cumulative Frequency Distribution
#+NAME: lst:xps_delay_burst_cdf
#+BEGIN_SRC python :results file :exports results
delay_plot("%s",
           series_linear_plot,
           'imgs/delay-impact-burst-cdf.svg',
           (XPS_DELAY_BURST
            .on_value(lambda df: df.sum(axis='columns'))
            .on_value(make_cumulative_frequency)
            .to_dict()),
           legend='all')
#+END_SRC

#+CAPTION: Impact of the Delay on the
#+CAPTION: Completion Time (Cumulative Frequency -- Burst).
#+ATTR_ORG: :width 100
#+RESULTS: lst:xps_delay_burst_cdf
[[file:imgs/delay-impact-burst-cdf.svg]]

* Do the size of the Database matter?
From
http://galeracluster.com/2016/08/optimized-state-snapshot-transfers-in-a-wan-environment/
#+BEGIN_QUOTE
If a node joins the cluster either for the first time or after a
period of prolonged downtime, it may need to obtain a complete
snapshot of the database from some other node. This operation is
called State Snapshot Transfer or SST, and is often reasonably quick
in a LAN environment.

In a geo-distributed cluster, however, the dataset may need to travel
over a slow WAN link. A transfer that takes seconds over a 10Gb
network can take hours over a cable modem.

SST does not happen during the normal operation of the cluster, but
may be needed during an outage situation which is already a stressful
time for the DevOps. During SST, the joining node is not available and
the donating node may be in a read-only state or have degraded
performance.
#+END_QUOTE

Note: CRDB may shine during commissioning over WAN. It could be cool
to add a test on that particular topic (ie, measuring the downtime
when commissioning a new node -- it should be 0 on CRDB).
#+CAPTION: Impact of the Cluster Size on the Completion Time (one Rally).
#+ATTR_ORG: :width 100
* Footer

#+BEGIN_EXPORT html
<script type="text/javascript">
$(document).ready( function () {
  $('.table-striped').DataTable({
    searching: false,
    stateSave: false,
    ordering: false,
    autowidth: false
  });

  $('.dataTables_length').hide();
});
</script>
#+END_EXPORT
